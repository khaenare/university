# Lab 06 — Parallel Algorithms of Solving Differential Equations in Partial Derivatives

## Мета роботи
Метою лабораторної роботи є ознайомлення з принципами розробки паралельних алгоритмів для розв’язання диференціальних рівнянь у частинних похідних (на прикладі задачі Діріхле для рівняння Пуассона).
У роботі реалізовано та досліджено:
- Послідовний алгоритм методу Гауса–Зейделя.
- Паралельний алгоритм на основі геометричного розпаралелювання (розбиття сітки на смуги) з використанням MPI.

---

## Опис реалізації

### 1. Послідовний алгоритм
Реалізовано класичний ітераційний метод Гауса–Зейделя на прямокутній сітці. Значення у кожному вузлі сітки $u_{i,j}$ перераховуються на основі значень сусідніх вузлів за шаблоном "хрест" (5-точковий шаблон):

$$u_{i,j}^{k+1} = 0.25 \cdot (u_{i-1,j}^{k+1} + u_{i+1,j}^{k} + u_{i,j-1}^{k+1} + u_{i,j+1}^{k} - h^2 f_{i,j})$$

Критерій зупинки: максимальна різниця значень між ітераціями менша за задану точність `Eps`.

### 2. Паралельний алгоритм
Використано одновимірну декомпозицію (block-striped decomposition). Матриця розбивається на горизонтальні смуги, які розподіляються між процесами.

**Ключові моменти реалізації:**
1.  **Розподіл даних:** Використовується функція `MPI_Scatterv` для передачі смуг різного розміру (оскільки N може не ділитися на кількість процесів націло).
2.  **Тіньові грані (Ghost Rows):** Кожен процес зберігає додаткові граничні рядки, які дублюють дані сусідніх процесів.
3.  **Комунікація:** На кожній ітерації процеси обмінюються граничними рядками за допомогою `MPI_Sendrecv`, щоб забезпечити цілісність даних.
4.  **Синхронізація точності:** Для перевірки умови зупинки використовується `MPI_Allreduce` з операцією `MPI_MAX`, щоб знайти максимальну похибку серед усіх процесів.
5.  **Збір результатів:** Фінальна матриця збирається на головному процесі через `MPI_Allgatherv` (або `MPI_Gatherv`).

---

## Результати експериментів

### Таблиця 6.1. Результати послідовного алгоритму

| Test Number | Matrix Size (N) | Iterations (k) | Time (sec) |
|:-----------:|:---------------:|:--------------:|:----------:|
| 1           | 10              | 44             | 0.000019   |
| 2           | 100             | 376            | 0.028836   |
| 3           | 1000            | 374            | 1.453066   |
| 4           | 2000            | 374            | 5.789990   |
| 5           | 3000            | 374            | 12.967154  |
| 6           | 4000            | 374            | 23.111325  |

### Таблиця 6.2. Порівняння експериментального та теоретичного часу
*Розраховано на основі опорного значення для N=2000.*

Конечно. [cite_start]Вот обновленная секция для **Таблицы 6.2**, куда я добавил теоретическое обоснование и формулы расчета согласно методическим указаниям[cite: 335, 337, 339].

Замени соответствующую часть в твоем `README.md` на этот текст:

### Таблиця 6.2. Порівняння експериментального та теоретичного часу

Теоретичний час виконання ($T_{theor}$) розраховується за формулою оцінки обчислювальної складності методу Гауса–Зейделя:

$$T_{theor} = k \cdot m \cdot N^2 \cdot \tau$$

де:
- $k$ — кількість виконаних ітерацій (з Таблиці 6.1);
- $m$ — кількість арифметичних операцій на один вузол сітки за одну ітерацію (для 5-точкового шаблону $m = 6$);
- $N$ — лінійний розмір сітки (кількість вузлів по одній координаті);
- $\tau$ — середній час виконання однієї базової операції.

Значення $\tau$ було визначено експериментально на основі опорного тесту з розміром сітки $N=2000$:

$$\tau = \frac{T_{exp}}{k \cdot m \cdot N^2} = \frac{5.789990}{374 \cdot 6 \cdot 2000^2} \approx 6.45 \cdot 10^{-10} \text{ сек}$$

| Test Number | Matrix Size | Execution Time (sec) | Theoretical Time (sec) |
|:-----------:|:-----------:|:--------------------:|:----------------------:|
| 1           | 10          | 0.000019             | 0.000017               |
| 2           | 100         | 0.028836             | 0.014600               |
| 3           | 1000        | 1.453066             | 1.448000               |
| 4           | 2000        | 5.789990             | 5.790000               |
| 5           | 3000        | 12.967154            | 13.027000              |
| 6           | 4000        | 23.111325            | 23.160000              |

### Таблиця 6.3. Результати паралельного алгоритму та прискорення (Speedup)

| Grid Size | Serial Time | 2 Processors (Time) | 2 Processors (Speedup) | 4 Processors (Time) | 4 Processors (Speedup) | 8 Processors (Time) | 8 Processors (Speedup) |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| **10** | 0.000019 | 0.001622 | 0.01 | 0.001451 | 0.01 | 0.002190 | 0.01 |
| **100** | 0.028836 | 0.008885 | 3.24 | 0.006825 | 4.22 | 0.005674 | 5.08 |
| **1000** | 1.453066 | 0.722798 | 2.01 | 0.396731 | 3.66 | 0.207250 | 7.01 |
| **2000** | 5.789990 | 2.922674 | 1.98 | 1.579866 | 3.66 | 0.831381 | 6.96 |
| **3000** | 12.967154 | 6.703419 | 1.93 | 3.543193 | 3.66 | 1.944470 | 6.67 |
| **4000** | 23.111325 | 12.019260 | 1.92 | 6.284256 | 3.68 | 3.371203 | 6.86 |
| **5000** | 36.136382 | 18.988596 | 1.90 | 9.788469 | 3.69 | 5.388406 | 6.71 |
| **6000** | 52.392176 | 27.284508 | 1.92 | 14.092507 | 3.72 | 7.692139 | 6.81 |
| **7000** | 71.608047 | 37.206246 | 1.92 | 19.230337 | 3.72 | 10.720597 | 6.68 |
| **8000** | 93.894300 | 48.928892 | 1.92 | 24.993678 | 3.76 | 14.140124 | 6.64 |
| **9000** | 119.350793 | 62.241176 | 1.92 | 31.727712 | 3.76 | 17.834509 | 6.69 |
| **10000**| 147.679438 | 76.480746 | 1.93 | 39.043817 | 3.78 | 22.229130 | 6.64 |

---

## Аналіз результатів

1.  **Малі розміри (N=10, 100):**
    На малих сітках паралельний алгоритм працює повільніше за послідовний (Speedup < 1). Це пояснюється тим, що час, витрачений на ініціалізацію MPI та обмін повідомленнями (latency), значно перевищує час корисних обчислень.

2.  **Великі розміри (N > 1000):**
    Зі зростанням розміру матриці спостерігається стійке прискорення.
    - **Для 2 процесорів:** прискорення наближається до 1.93 (теоретичний максимум — 2).
    - **Для 4 процесорів:** прискорення сягає 3.78 (теоретичний максимум — 4).
    - **Для 8 процесорів:** прискорення становить близько 6.7–6.8 (теоретичний максимум — 8).

3.  **Ефективність:**
    Прискорення не досягає ідеального лінійного значення через накладні витрати на передачу граничних рядків (`MPI_Sendrecv`) та синхронізацію (`MPI_Allreduce`). Однак, для великих задач ефективність залишається високою, що підтверджує доцільність використання паралельних обчислень для розв’язання задач математичної фізики.