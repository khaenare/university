
# Lab 02 — Parallel Matrix–Matrix Multiplication (MPI, Fox Algorithm)

##  Мета роботи
Метою лабораторної є ознайомлення з принципами **паралельного множення матриць** за допомогою бібліотеки **MPI** (Message Passing Interface) та алгоритму **Fox**.  
У ході роботи реалізовано **послідовну** та **паралельну** версії множення квадратних матриць, проведено експериментальні вимірювання часу виконання, обчислено **прискорення (Speedup)** та проведено порівняння з теоретичною моделлю.

---

##  Технічні вимоги
* **Мова:** C++
* **Бібліотека:** MPI (OpenMPI)
* **Компілятор:** `mpic++` (для MPI), `g++` (для Serial)
* **Середовище:** macOS

---

##  Інструкція із запуску

### 1. Компіляція
**Послідовна версія:**
```bash
g++ SerialMM.cpp -o SerialMM
````

**Паралельна версія:**

```bash
mpic++ ParallelMM.cpp -o ParallelMM
```

### 2\. Запуск

**Послідовна версія:**

```bash
./SerialMM
```

**Паралельна версія:**
Необхідно вказати кількість процесів (`-n`), яка є повним квадратом (4, 9, 16...).

```bash
mpiexec -n 4 ./ParallelMM
```

*(Примітка: для запуску на 9 процесорах на локальній машині може знадобитися прапорець `--oversubscribe`)*

-----

##  Теоретичні відомості

### Послідовний алгоритм

Стандартне множення матриць ($C = A \times B$) з часовою складністю **O(n³)**.

### Паралельний алгоритм (Fox Algorithm)

Використовує блочну декомпозицію (grid topology $q \times q$, де $q=\sqrt{p}$).
Теоретична оцінка часу виконання паралельного алгоритму ($T_p$) описується формулою:

$$
T_p = q \left[ \frac{n^2}{p} \left( \frac{2n}{q} - 1 \right) + \frac{n^2}{p} \right] \tau + (q \log_2 q + (q-1)) \left( \alpha + \frac{w n^2}{p \beta} \right)
$$

Де:

  - $n$ — розмір матриці
  - $p$ — кількість процесорів
  - $\tau$ — час однієї елементарної операції
  - $\alpha$ — латентність мережі
  - $\beta$ — пропускна здатність мережі

-----

##  Результати експериментів

### Таблиця 1. Час виконання та прискорення

Отримані результати виконання програм (час у секундах):

| Matrix Size | Serial Algorithm (sec) | 4 processors Time | 4 processors Speedup | 9 processors Time | 9 processors Speedup |
| :---: | :---: | :---: | :---: | :---: | :---: |
| **10** | 0.000005 | 0.000470 | 0.01 | 0.001166 | 0.004 |
| **100** | 0.002341 | 0.001399 | 1.67 | 0.001124 | 2.08 |
| **500** | 0.181707 | 0.062052 | 2.93 | 0.036498 | 4.98 |
| **1000** | 1.531603 | 0.405472 | 3.78 | 0.215515 | 7.11 |
| **1500** | 5.550211 | 1.468839 | 3.78 | 0.721749 | 7.69 |
| **2000** | 14.808957 | 3.387429 | 4.37 | 1.682398 | 8.80 |
| **2500** | 36.183247 | 7.665046 | 4.72 | 3.606312 | 10.03 |
| **3000** | 63.840061 | 12.526955 | **5.10** | 5.864958 | **10.88** |

### Таблиця 2. Порівняння теоретичного та експериментального часу

Порівняння реального часу з ідеальною моделлю ($T_{model} = T_{serial} / p$).

| Test Number | Matrix Sizes | 4 processors Model (sec) | 4 processors Experiment (sec) | 9 processors Model (sec) | 9 processors Experiment (sec) |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 1 | **10** | 0.000001 | 0.000470 | 0.0000005 | 0.001166 |
| 2 | **100** | 0.000585 | 0.001399 | 0.000260 | 0.001124 |
| 3 | **500** | 0.045427 | 0.062052 | 0.020190 | 0.036498 |
| 4 | **1000** | 0.382901 | 0.405472 | 0.170178 | 0.215515 |
| 5 | **1500** | 1.387553 | 1.468839 | 0.616690 | 0.721749 |
| 6 | **2000** | 3.702239 | 3.387429 | 1.645440 | 1.682398 |
| 7 | **2500** | 9.045812 | 7.665046 | 4.020361 | 3.606312 |
| 8 | **3000** | 15.960015 | **12.526955** | 7.093340 | **5.864958** |

-----

##  Висновки та Аналіз

1.  **Малі матриці (N \< 500):**
    Паралельний алгоритм працює повільніше або ненабагато швидше за теоретичну модель. Це пояснюється тим, що час на комунікацію між процесами (ініціалізація MPI, пересилання блоків) займає значну частину загального часу виконання, перекриваючи виграш від розпаралелювання обчислень.

2.  **Великі матриці (N \> 2000):**
    Спостерігається **надлінійне прискорення** (Super-linear Speedup).

      * Для 4 процесорів прискорення досягає **5.10** (теоретичний максимум — 4).
      * Для 9 процесорів прискорення досягає **10.88** (теоретичний максимум — 9).
      * Експериментальний час виявився меншим за модельний.

    **Причина ефекту:** При розбитті великої матриці на блоки (наприклад, $3000 \times 3000$ розбивається на менші підматриці), ці менші блоки повністю поміщаються в швидку кеш-пам'ять процесора (L2/L3 Cache). У послідовному алгоритмі вся матриця не влазить у кеш, що призводить до частого і повільного звернення до оперативної пам'яті (RAM).

3.  **Загальний підсумок:**
    Алгоритм Фокса демонструє відмінну масштабованість. Ефективність паралелізації зростає зі збільшенням обсягу задачі. Використання MPI дозволяє значно скоротити час обчислень для великих матриць, обганяючи навіть ідеальні теоретичні прогнози завдяки ефективнішому використанню ієрархії пам'яті.

