

# Лабораторна робота №5: Паралельні алгоритми обробки графів (Алгоритм Флойда)

##  Мета роботи
Розробка та дослідження ефективності паралельної версії алгоритму Флойда для пошуку найкоротших шляхів між усіма парами вершин у зваженому графі. Аналіз прискорення обчислень при використанні технології MPI порівняно з послідовною версією.

---

##  Опис реалізації

### 1. Послідовний алгоритм
Базується на класичному алгоритмі Флойда-Воршелла.
- **Структура даних:** Матриця суміжності розміром $N \times N$, розгорнута в одновимірний масив.
- **Логіка:** Три вкладені цикли перебирають усі пари вершин $(i, j)$ через проміжну вершину $k$.
- **Складність:** $O(N^3)$.

### 2. Паралельний алгоритм (MPI)
Використовується **стрічкова схема розбиття даних (1D decomposition)** по рядках.

**Ключові етапи:**
1.  **Декомпозиція даних:**
    * Головний процес (Rank 0) генерує матрицю.
    * Використовується `MPI_Scatterv` для розподілу рядків матриці між процесами. Це дозволяє обробляти ситуації, коли $N$ не ділиться на кількість процесів націло (використовуються масиви зміщень `Displs` та лічильників `SendCounts`).
2.  **Основний цикл:**
    * На кожній ітерації $k$ (де $k$ — проміжна вершина) процес, що володіє $k$-м рядком, розсилає його всім іншим процесам за допомогою `MPI_Bcast`.
    * Кожен процес оновлює лише свою частину рядків, використовуючи отриманий $k$-й рядок. Це мінімізує обсяг комунікацій порівняно з пересилкою всієї матриці.
3.  **Збір результатів:**
    * Після завершення обчислень оновлені смуги рядків збираються на головному процесі за допомогою `MPI_Gatherv`.

---

##  Результати експериментів

Експерименти проводилися на графі з кількістю вершин від 10 до 1000. Вимірювався час виконання послідовної версії та паралельної на 2, 4 та 8 процесах.

### Таблиця 1. Часові показники та прискорення (Speedup)

| Кількість вершин | Serial (сек) | 2 Proc (сек) | Speedup (2) | 4 Proc (сек) | Speedup (4) | 8 Proc (сек) | Speedup (8) |
|------------------|--------------|--------------|-------------|--------------|-------------|--------------|-------------|
| **10** | 0.000027     | 0.000062     | 0.44        | 0.000077     | 0.35        | 0.000154     | 0.18        |
| **500** | 0.492589     | 0.241495     | 2.04        | 0.129114     | 3.82        | 0.070264     | 7.01        |
| **600** | 0.825813     | 0.408834     | 2.02        | 0.217209     | 3.80        | 0.114593     | 7.21        |
| **700** | 1.290147     | 0.640523     | 2.01        | 0.341199     | 3.78        | 0.180615     | 7.14        |
| **800** | 1.912158     | 0.953660     | 2.01        | 0.501858     | 3.81        | 0.260751     | 7.33        |
| **900** | 2.708720     | 1.355483     | 1.99        | 0.708017     | 3.83        | 0.365573     | 7.41        |
| **1000** | 3.695043     | 1.850644     | 1.99        | 0.964795     | 3.83        | 0.497179     | 7.43        |

---

##  Порівняння з теоретичною моделлю

### Таблиця 2. Порівняння експериментального та теоретичного часу послідовного алгоритму

Розрахунок теоретичного часу базується на формулі $T = Size^3 \cdot \tau$.
За опорне значення для розрахунку $\tau$ взято експеримент із **800 вершинами**.

**Обчислене значення $\tau$:** $3.73468 \cdot 10^{-9}$ сек.

| Test Number | Vertices | Execution Time (sec) | Theoretical Time (sec) |
|-------------|----------|----------------------|------------------------|
| 1           | 10       | 0.000027             | 0.000004               |
| 2           | 500      | 0.492589             | 0.466835               |
| 3           | 600      | 0.825813             | 0.806691               |
| 4           | 700      | 1.290147             | 1.281000               |
| 5           | 800      | 1.912158             | 1.912158               |
| 6           | 900      | 2.708720             | 2.722680               |
| 7           | 1000     | 3.695043             | 3.734680               |

### Порівняння експериментального та теоретичного часу
Для оцінки ефективності використано модель ідеального лінійного прискорення:
$$T_{model} = \frac{T_{serial}}{P}$$
де $P$ — кількість процесів.

### Таблиця 2. Порівняння експериментального та теоретичного часу

| Вершини | 2 Proc (Model) | 2 Proc (Exp) | 4 Proc (Model) | 4 Proc (Exp) | 8 Proc (Model) | 8 Proc (Exp) |
|---------|----------------|--------------|----------------|--------------|----------------|--------------|
| **10** | 0.000014       | 0.000062     | 0.000007       | 0.000077     | 0.000003       | 0.000154     |
| **500** | 0.246295       | 0.241495     | 0.123147       | 0.129114     | 0.061574       | 0.070264     |
| **800** | 0.956079       | 0.953660     | 0.478040       | 0.501858     | 0.239020       | 0.260751     |
| **1000**| 1.847522       | 1.850644     | 0.923761       | 0.964795     | 0.461880       | 0.497179     |

---

##  Висновки

1.  **Малі графи (10 вершин):** Спостерігається "сповільнення" (Speedup < 1). Це очікувана поведінка, оскільки накладні витрати на ініціалізацію MPI та комунікацію (пересилку даних мережею/пам'яттю) значно перевищують час корисних обчислень, який для 10 вершин є мізерним.
2.  **Великі графи (500-1000 вершин):** Ефективність паралелізації різко зростає.
    * Для **1000 вершин** прискорення на 8 процесорах становить **7.43**, що дуже близьке до ідеального значення (8.0).
    * Це пояснюється тим, що обчислювальна складність зростає як $O(N^3)$, а комунікаційна складність (передача рядків) зростає повільніше — як $O(N^2)$. Тобто з ростом $N$ частка корисних обчислень збільшується відносно часу комунікацій.
3.  **Відхилення від моделі:** Експериментальний час трохи більший за теоретичний на 4 та 8 процесах через витрати на синхронізацію (`MPI_Bcast`) та нерівномірність доступу до пам'яті, проте результати демонструють високу ефективність розробленого алгоритму для задач великої розмірності.